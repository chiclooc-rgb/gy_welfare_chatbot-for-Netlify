# ingest.py
import os, json, re, glob
from pathlib import Path
# from tqdm import tqdm  # tqdm ëŒ€ì‹  ê°„ë‹¨í•œ ì§„í–‰ í‘œì‹œ ì‚¬ìš©
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from markdown import markdown
from bs4 import BeautifulSoup  # markdown -> text ì •ì œë¥¼ ìœ„í•œ ë³´ì¡°
import pandas as pd
import docx

DATA_DIR = Path("data")
INDEX_DIR = Path("index")
INDEX_DIR.mkdir(exist_ok=True)

EMB_MODEL_NAME = "jhgan/ko-sroberta-multitask"
CHUNK_SIZE = 1000   # ë¬¸ì ê¸°ì¤€(ê°„ë‹¨), í•„ìš” ì‹œ í† í°í™” ê¸°ë°˜ìœ¼ë¡œ ê°œì„ 
CHUNK_OVERLAP = 200

def read_txt(path: Path) -> str:
    """í…ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸°"""
    return path.read_text(encoding="utf-8", errors="ignore")

def read_md(path: Path) -> str:
    """ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì½ê¸° ë° HTML ë³€í™˜ í›„ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    raw = path.read_text(encoding="utf-8", errors="ignore")
    # markdown â†’ html â†’ text
    html = markdown(raw)
    text = BeautifulSoup(html, "html.parser").get_text("\n")
    return text

def read_docx(path: Path) -> str:
    """Word ë¬¸ì„œ ì½ê¸°"""
    doc = docx.Document(str(path))
    paras = [p.text.strip() for p in doc.paragraphs if p.text.strip()]
    return "\n".join(paras)

def read_xlsx(path: Path) -> str:
    """Excel íŒŒì¼ ì½ê¸° - ì‹œíŠ¸ë³„ë¡œ í‘œë¥¼ ê°„ë‹¨íˆ í…ìŠ¤íŠ¸ë¡œ ì§ë ¬í™”"""
    xls = pd.ExcelFile(path)
    out = []
    for sheet in xls.sheet_names:
        df = xls.parse(sheet).fillna("")
        out.append(f"[Sheet] {sheet}")
        # í—¤ë” í¬í•¨ row ë‹¨ìœ„ ì§ë ¬í™”
        cols = [str(c) for c in df.columns]
        out.append(" | ".join(cols))
        for _, row in df.iterrows():
            out.append(" | ".join(str(x) for x in row.tolist()))
    return "\n".join(out)

def load_and_extract(path: Path) -> str:
    """íŒŒì¼ í™•ì¥ìì— ë”°ë¼ ì ì ˆí•œ íŒŒì„œë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    ext = path.suffix.lower()
    if ext == ".txt":
        return read_txt(path)
    if ext == ".md":
        return read_md(path)
    if ext == ".docx":
        return read_docx(path)
    if ext == ".xlsx":
        return read_xlsx(path)
    return ""  # ë¹„ëŒ€ìƒ í¬ë§·ì€ ìŠ¤í‚µ

def chunk_text(text: str, size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):
    """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„í• """
    text = re.sub(r"\n{3,}", "\n\n", text).strip()
    chunks = []
    start = 0
    while start < len(text):
        end = start + size
        chunk = text[start:end]
        chunks.append(chunk)
        if end >= len(text):
            break
        start = end - overlap
    return chunks

def main():
    """ë©”ì¸ ì¸ë±ì‹± íŒŒì´í”„ë¼ì¸"""
    print("ë¬¸ì„œ ìˆ˜ì§‘ ë° íŒŒì‹± ì‹œì‘...")
    
    # ì§€ì›í•˜ëŠ” íŒŒì¼ í˜•ì‹ë“¤ ìˆ˜ì§‘
    files = []
    for ext in ("*.txt","*.md","*.docx","*.xlsx"):
        files.extend(glob.glob(str(DATA_DIR / ext)))
    files = [Path(f) for f in files]
    
    if not files:
        print(f"âŒ {DATA_DIR} ë””ë ‰í„°ë¦¬ì— ì§€ì›í•˜ëŠ” íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        print("ì§€ì› í˜•ì‹: .txt, .md, .docx, .xlsx")
        return

    print(f"ğŸ“ ë°œê²¬ëœ íŒŒì¼: {len(files)}ê°œ")
    for f in files:
        print(f"  - {f.name}")

    # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
    print(f"ğŸ¤– ì„ë² ë”© ëª¨ë¸ ë¡œë”©: {EMB_MODEL_NAME}")
    model = SentenceTransformer(EMB_MODEL_NAME)
    
    vectors = []
    metadatas = []
    corpus_texts = []

    uid = 0
    for i, f in enumerate(files):
        print(f"íŒŒì‹± ì¤‘... ({i+1}/{len(files)}) {f.name}")
        text = load_and_extract(f)
        if not text: 
            print(f"âš ï¸  {f.name}: í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨")
            continue
            
        chunks = chunk_text(text)
        print(f"ğŸ“„ {f.name}: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
        
        for i, ch in enumerate(chunks):
            meta = {
                "uid": uid,
                "source": f.name,
                "chunk_id": i,
                "path": str(f.resolve())
            }
            corpus_texts.append(ch)
            metadatas.append(meta)
            uid += 1

    print(f"âœ… ì´ ì²­í¬ ìˆ˜: {len(corpus_texts)}")
    if not corpus_texts:
        print("âŒ ì²­í¬ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ./dataì— íŒŒì¼ì„ ë„£ê³  ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.")
        return

    print("ğŸ”„ ì„ë² ë”© ìƒì„± ì¤‘...")
    emb = model.encode(corpus_texts, batch_size=64, show_progress_bar=False, normalize_embeddings=True)
    emb = np.array(emb).astype("float32")

    print("ğŸ” FAISS ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
    dim = emb.shape[1]
    index = faiss.IndexFlatIP(dim)  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ìš©(ì •ê·œí™”í–ˆìœ¼ë¯€ë¡œ ë‚´ì )
    index.add(emb)

    # ì¸ë±ìŠ¤ ì €ì¥
    faiss.write_index(index, str(INDEX_DIR / "faiss.index"))
    with open(INDEX_DIR / "meta.json", "w", encoding="utf-8") as f:
        json.dump({"metas": metadatas, "texts": corpus_texts}, f, ensure_ascii=False)

    print("âœ… ì™„ë£Œ! ì¸ë±ìŠ¤ê°€ ./indexì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"ğŸ“Š í†µê³„:")
    print(f"  - ì´ ë¬¸ì„œ: {len(files)}ê°œ")
    print(f"  - ì´ ì²­í¬: {len(corpus_texts)}ê°œ")
    print(f"  - ì„ë² ë”© ì°¨ì›: {dim}")

if __name__ == "__main__":
    main()