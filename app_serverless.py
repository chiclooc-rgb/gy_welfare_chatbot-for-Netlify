# app_serverless.py - Netlify Functions ìµœì í™” ë²„ì „
import os, json, asyncio
from pathlib import Path
import faiss
import numpy as np
from fastapi import FastAPI, HTTPException
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from pydantic import BaseModel
from sentence_transformers import SentenceTransformer
from dotenv import load_dotenv
from openai import OpenAI
import uuid
from datetime import datetime
from typing import List, Dict, Optional
from contextlib import asynccontextmanager

# ì„œë²„ë¦¬ìŠ¤ í™˜ê²½ ê°ì§€
IS_SERVERLESS = os.environ.get('AWS_LAMBDA_FUNCTION_NAME') or os.environ.get('NETLIFY')

# --- ì±—ë´‡ì˜ í•µì‹¬ ìì›(ëª¨ë¸, ì¸ë±ìŠ¤)ì„ ê´€ë¦¬í•˜ëŠ” lifespan í•¨ìˆ˜ ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    # ì„œë²„ê°€ ì‹œì‘ë  ë•Œ ì‹¤í–‰ë˜ëŠ” ë¶€ë¶„
    print("ğŸš€ ì„œë²„ ì‹œì‘! ì¸ë±ìŠ¤ì™€ ëª¨ë¸ì„ ë¹„ë™ê¸°ì ìœ¼ë¡œ ë¡œë“œí•©ë‹ˆë‹¤...")
    # ë¬´ê±°ìš´ ì‘ì—…ì„ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ì„œë²„ ì‹œì‘ì„ ë°©í•´í•˜ì§€ ì•ŠìŒ
    await load_resources()
    
    # ì„œë²„ë¦¬ìŠ¤ í™˜ê²½ì—ì„œëŠ” íŒŒì¼ ì›Œì³ ë¹„í™œì„±í™”
    if not IS_SERVERLESS:
        try:
            from file_watcher import init_file_watcher, start_file_watcher, stop_file_watcher
            print("ğŸ‘ï¸ íŒŒì¼ ì›Œì³ ì´ˆê¸°í™” ì¤‘...")
            init_file_watcher(DATA_DIR, add_documents_to_index)
            start_file_watcher()
            print("âœ… íŒŒì¼ ì›Œì³ ì‹œì‘ë¨")
        except ImportError:
            print("âš ï¸ íŒŒì¼ ì›Œì³ ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print("ğŸ“¦ ì„œë²„ë¦¬ìŠ¤ í™˜ê²½ì—ì„œëŠ” íŒŒì¼ ì›Œì³ê°€ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
    
    yield
    # ì„œë²„ê°€ ì¢…ë£Œë  ë•Œ ì‹¤í–‰ë˜ëŠ” ë¶€ë¶„ (ì •ë¦¬ ì½”ë“œ)
    if not IS_SERVERLESS:
        try:
            from file_watcher import stop_file_watcher
            print("ğŸ›‘ íŒŒì¼ ì›Œì³ ì¤‘ì§€ ì¤‘...")
            stop_file_watcher()
        except ImportError:
            pass
    print("ğŸ‘‹ ì„œë²„ ì¢…ë£Œ.")

async def load_resources():
    """AI ëª¨ë¸ ë° ì¸ë±ìŠ¤ íŒŒì¼ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜"""
    global index, metas, texts, emb_model
    try:
        index = faiss.read_index(str(INDEX_DIR / "faiss.index"))
        with open(INDEX_DIR / "meta.json", "r", encoding="utf-8") as f:
            store = json.load(f)
        metas = store["metas"]
        texts = store["texts"]
        emb_model = SentenceTransformer(EMB_MODEL_NAME)
        print("âœ… ì¸ë±ìŠ¤ì™€ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
    except Exception as e:
        print(f"âŒ ì¸ë±ìŠ¤/ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# API í‚¤ ì„¤ì • - í™˜ê²½ë³€ìˆ˜ì—ì„œë§Œ ë¡œë“œ (ë³´ì•ˆìƒ í•˜ë“œì½”ë”© ê¸ˆì§€)
OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')

if not OPENAI_API_KEY or OPENAI_API_KEY == "YOUR_API_KEY_HERE":
    print("âš ï¸ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    client = None
else:
    client = OpenAI(api_key=OPENAI_API_KEY)

INDEX_DIR = Path("index")
DATA_DIR = Path("data")
EMB_MODEL_NAME = "jhgan/ko-sroberta-multitask"
TOP_K = 10
SIMILARITY_THRESHOLD = 0.1

# í‚¤ì›Œë“œ í™•ì¥ ë§µ
KEYWORD_EXPANSION = {
    "ë‹¤ìë…€ê°€ì •": ["ë‹¤ìë…€", "ì…‹ì§¸ì•„ì´", "3ìë…€", "3ëª… ì´ìƒ", "ë§ì€ ìë…€"], 
    "ë‹¤ìë…€": ["ë‹¤ìë…€ê°€ì •", "ì…‹ì§¸ì•„ì´", "3ìë…€", "3ëª… ì´ìƒ"],
    "í˜œíƒ": ["ì§€ì›", "ë³´ì¡°", "ê¸‰ì—¬", "ìˆ˜ë‹¹", "í• ì¸", "ê°ë©´", "ìš°ëŒ€"], 
    "ì§€ì›": ["í˜œíƒ", "ë³´ì¡°", "ê¸‰ì—¬", "ìˆ˜ë‹¹", "ì§€ì›ê¸ˆ"],
    "ì„ì‹ ": ["ì„ì‚°ë¶€", "ì˜ˆë¹„ë§˜", "ì‚°ëª¨", "ì„ì‹ ë¶€"], 
    "ì¶œì‚°": ["ë¶„ë§Œ", "í•´ì‚°", "ì‹ ìƒì•„", "ì‚°í›„ì¡°ë¦¬"],
    "ìœ¡ì•„": ["ì–‘ìœ¡", "ìë…€ëŒë´„", "ë³´ìœ¡", "ìœ¡ì•„íœ´ì§"], 
    "ë³´ìœ¡": ["ì–´ë¦°ì´ì§‘", "ìœ ì¹˜ì›", "ë†€ì´ë°©", "ìœ¡ì•„", "ì–‘ìœ¡"],
    "í•œë¶€ëª¨": ["í•œë¶€ëª¨ê°€ì •", "ë¯¸í˜¼ëª¨", "í¸ë¶€ëª¨", "ì¡°ì†ê°€ì •"]
}

# === ì¸ë±ìŠ¤ ë° ëª¨ë¸ ë³€ìˆ˜ ì´ˆê¸°í™” ===
index = None
metas = []
texts = []
emb_model = None

# FastAPI ì•± ìƒì„± ì‹œ lifespan ì—°ê²°
app = FastAPI(
    title="ì§€ëŠ¥í˜• ë³µì§€ ìƒë‹´ ì±—ë´‡",
    description="ë¬¸ì„œ ê¸°ë°˜ì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” RAG ì±—ë´‡ì…ë‹ˆë‹¤.",
    lifespan=lifespan
)

# ì •ì  íŒŒì¼ ì„œë¹™ (ì„œë²„ë¦¬ìŠ¤ì—ì„œëŠ” ì œí•œì )
if not IS_SERVERLESS:
    app.mount("/static", StaticFiles(directory="frontend"), name="static")
    app.mount("/admin", StaticFiles(directory="admin"), name="admin")

# ê´€ë¦¬ì ì„¤ì •
ADMIN_PASSWORD = os.environ.get('ADMIN_PASSWORD', "admin123")

def verify_admin_password(x_admin_password: Optional[str] = None):
    """ê´€ë¦¬ì ê¶Œí•œ í™•ì¸"""
    if x_admin_password != ADMIN_PASSWORD:
        raise HTTPException(status_code=403, detail="ê´€ë¦¬ì ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤")
    return True

# ì„¸ì…˜ ê´€ë¦¬
class ConversationMessage(BaseModel):
    role: str
    content: str
    timestamp: datetime
    sources: Optional[List[Dict]] = None

class ConversationSession:
    def __init__(self, session_id: str):
        self.session_id = session_id
        self.messages: List[ConversationMessage] = []
        self.created_at = datetime.now()
    
    def add_message(self, role: str, content: str, sources: Optional[List[Dict]] = None):
        self.messages.append(ConversationMessage(
            role=role, 
            content=content, 
            timestamp=datetime.now(), 
            sources=sources
        ))
    
    def get_context(self, max_messages: int = 4) -> str:
        """ìµœê·¼ ëŒ€í™” ë‚´ìš©ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ ë°˜í™˜"""
        recent_messages = self.messages[-max_messages:]
        context_parts = []
        for msg in recent_messages:
            if msg.role == "user":
                context_parts.append(f"ì‚¬ìš©ì: {msg.content}")
            else:
                context_parts.append(f"ìƒë‹´ì‚¬: {msg.content}")
        return "\n".join(context_parts)

sessions: Dict[str, ConversationSession] = {}

# API ëª¨ë¸
class AskReq(BaseModel):
    question: str
    session_id: Optional[str] = None

class NewSessionResponse(BaseModel):
    session_id: str
    message: str

def expand_query(query: str) -> str:
    """ì§ˆë¬¸ì„ í™•ì¥í•˜ì—¬ ë” ë‚˜ì€ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì–»ê¸°"""
    expanded_terms = []
    query_lower = query.lower()
    
    for keyword, synonyms in KEYWORD_EXPANSION.items():
        if keyword in query_lower:
            expanded_terms.extend(synonyms)
        for synonym in synonyms:
            if synonym in query_lower and keyword not in expanded_terms:
                expanded_terms.append(keyword)
                expanded_terms.extend([s for s in synonyms if s != synonym])
    
    if expanded_terms:
        unique_terms = list(set(expanded_terms))
        return f"{query} {' '.join(unique_terms[:5])}"
    return query

def search_similar(query: str, k=TOP_K):
    """ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤"""
    if not index or not emb_model:
        raise HTTPException(status_code=503, detail="ëª¨ë¸/ì¸ë±ìŠ¤ê°€ ì•„ì§ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
    
    # ì§ˆë¬¸ í™•ì¥
    expanded_query = expand_query(query)
    
    # ì„ë² ë”© ë° ê²€ìƒ‰
    q_emb = emb_model.encode([expanded_query], normalize_embeddings=True).astype("float32")
    D, I = index.search(q_emb, k)
    
    results = []
    for i, score in zip(I[0], D[0]):
        if score >= SIMILARITY_THRESHOLD:  # ìœ ì‚¬ë„ ì„ê³„ê°’ ì´ìƒë§Œ í¬í•¨
            results.append({
                "text": texts[i],
                "source": metas[i]["source"],
                "chunk_id": metas[i]["chunk_id"],
                "score": float(score)
            })
    
    return results

SYSTEM_PROMPT = (
    "ë‹¹ì‹ ì€ ìƒì• ë³µì§€í”Œë«í¼ì˜ ì „ë¬¸ ë³µì§€ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ì œê³µëœ 'ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸'ë¥¼ ì§€ëŠ¥ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ë‹µë³€í•˜ì‹­ì‹œì˜¤.\n\n"
    "## ğŸ” ì •ì±… ë¶„ë¥˜ ë° ìš°ì„ ìˆœìœ„ ë¶„ì„ ë°©ë²•:\n"
    "1. **ì „ìš© ì •ì±…**: íŠ¹ì • ëŒ€ìƒ(ë‹¤ìë…€ê°€ì •, í•œë¶€ëª¨ê°€ì • ë“±)ë§Œì„ ìœ„í•œ ì „ìš© ì§€ì› ì •ì±…\n"
    "2. **ìš°ëŒ€ ì •ì±…**: ì¼ë°˜ ì •ì±…ì—ì„œ íŠ¹ì • ëŒ€ìƒì—ê²Œ ìš°ëŒ€ í˜œíƒì„ ì œê³µí•˜ëŠ” ì •ì±…\n"
    "3. **ê´€ë ¨ ì •ì±…**: ê°„ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ ì •ì±…\n\n"
    "## ğŸ“‹ ë‹µë³€ êµ¬ì¡°í™” ì§€ì¹¨:\n"
    "**ë‹¤ìë…€ê°€ì • ì§€ì›ì •ì±…** ì§ˆë¬¸ ì‹œ:\n"
    "- ğŸ¯ **ë‹¤ìë…€ê°€ì • ì „ìš© ì •ì±…** (ìµœìš°ì„ )\n"
    "- ğŸ”– **ë‹¤ìë…€ê°€ì • ìš°ëŒ€ í˜œíƒ** (ì°¨ìˆœìœ„)\n\n"
    "**ì„ì‹ Â·ì¶œì‚° ì§€ì›ì •ì±…** ì§ˆë¬¸ ì‹œ:\n"
    "- ğŸ¯ **ì„ì‹ Â·ì¶œì‚° ì „ìš© ì •ì±…** (ìµœìš°ì„ )\n"
    "- ğŸ”– **ì„ì‹ Â·ì¶œì‚° ìš°ëŒ€ í˜œíƒ** (ì°¨ìˆœìœ„)\n\n"
    "**í•œë¶€ëª¨ê°€ì • ì§€ì›ì •ì±…** ì§ˆë¬¸ ì‹œ:\n"
    "- ğŸ¯ **í•œë¶€ëª¨ê°€ì • ì „ìš© ì •ì±…** (ìµœìš°ì„ )\n"
    "- ğŸ”– **í•œë¶€ëª¨ê°€ì • ìš°ëŒ€ í˜œíƒ** (ì°¨ìˆœìœ„)\n\n"
    "## âœ… ë‹µë³€ ê·œì¹™:\n"
    "- ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ ì•ˆì—ì„œë§Œ ë‹µë³€í•˜ê³ , ì •ì±…ì„ ë¶„ë¥˜ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ì œì‹œí•˜ì„¸ìš”.\n"
    "- ê° ì •ì±…ì˜ ëŒ€ìƒ, ë‚´ìš©, ì‹ ì²­ë°©ë²•ì„ ëª…í™•íˆ ìš”ì•½í•˜ê³ , ë¬¸ì¥ë§ˆë‹¤ [ì¶œì²˜: íŒŒì¼ëª…#ì²­í¬]ë¥¼ í‘œê¸°í•˜ì„¸ìš”.\n"
    "- ì „ìš© ì •ì±…ì„ ìš°ì„ ì ìœ¼ë¡œ ìƒì„¸íˆ ì„¤ëª…í•˜ê³ , ìš°ëŒ€ ì¡°ê±´ì€ ë³„ë„ë¡œ êµ¬ë¶„í•˜ì—¬ ì„¤ëª…í•˜ì„¸ìš”.\n"
    "- ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì€ 'ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.\n"
    "- ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ê°€ë…ì„± ìˆê²Œ ì‘ì„±í•˜ì„¸ìš”.\n"
    "- ë‹µë³€ ë§ˆì§€ë§‰ì— ì¶”ê°€ ì§ˆë¬¸ì„ ìœ ë„í•˜ëŠ” ì¹œê·¼í•œ ë©˜íŠ¸ë¥¼ í¬í•¨í•˜ì„¸ìš”."
)

def build_prompt(question: str, contexts: list[dict], conversation_history: str = ""):
    """ì§ˆë¬¸ê³¼ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ LLM í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±"""
    ctx_blocks = [f"[{r['source']}#{r['chunk_id']}]\n{r['text']}\n" for r in contexts]
    ctx_text = "\n---\n".join(ctx_blocks)
    
    instruction = (
        "ì§ˆë¬¸ì— ë§ëŠ” ì •í™•í•œ ì •ë³´ë¥¼ ì°¾ì•„ì„œ, ì •ì±… ì¢…ë¥˜(ì „ìš©/ìš°ëŒ€/ê´€ë ¨)ì— ë”°ë¼ ë¶„ë¥˜í•˜ê³  "
        "ìš°ì„ ìˆœìœ„ì— ë§ê²Œ ìƒì„¸íˆ ì„¤ëª…í•˜ì„¸ìš”."
    )
    
    user_parts = []
    if conversation_history.strip():
        user_parts.append(f"[ì´ì „ ëŒ€í™”]\n{conversation_history}\n")
    
    user_parts.extend([
        f"[í˜„ì¬ ì§ˆë¬¸]\n{question}\n",
        f"[ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸]\n{ctx_text}\n",
        f"[ì§€ì‹œì‚¬í•­]\n{instruction}\n"
    ])
    
    return "\n".join(user_parts)

@app.get("/")
async def read_root():
    if IS_SERVERLESS:
        return {"message": "ë³µì§€ ìƒë‹´ ì±—ë´‡ API", "status": "running", "environment": "serverless"}
    return FileResponse("frontend/index.html")

@app.post("/new-session")
def create_new_session():
    """ìƒˆë¡œìš´ ëŒ€í™” ì„¸ì…˜ì„ ìƒì„±í•©ë‹ˆë‹¤"""
    session_id = str(uuid.uuid4())
    sessions[session_id] = ConversationSession(session_id)
    return NewSessionResponse(
        session_id=session_id,
        message="ìƒˆë¡œìš´ ìƒë‹´ ì„¸ì…˜ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. ê¶ê¸ˆí•œ ë³µì§€ ì •ì±…ì— ëŒ€í•´ ì§ˆë¬¸í•´ì£¼ì„¸ìš”!"
    )

@app.get("/sessions")
def list_sessions():
    """í˜„ì¬ í™œì„± ì„¸ì…˜ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤"""
    return {
        "sessions": [
            {
                "session_id": session.session_id,
                "created_at": session.created_at,
                "message_count": len(session.messages)
            }
            for session in sessions.values()
        ]
    }

@app.get("/session/{session_id}/history")
def get_session_history(session_id: str):
    """íŠ¹ì • ì„¸ì…˜ì˜ ëŒ€í™” ê¸°ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤"""
    if session_id not in sessions:
        raise HTTPException(status_code=404, detail="ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    session = sessions[session_id]
    return {
        "session_id": session_id,
        "messages": [
            {
                "role": msg.role,
                "content": msg.content,
                "timestamp": msg.timestamp,
                "sources": msg.sources
            }
            for msg in session.messages
        ]
    }

@app.get("/documents")
def get_indexed_documents():
    """ì¸ë±ìŠ¤ëœ ë¬¸ì„œ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤"""
    if not metas:
        return {"documents": [], "total_documents": 0, "total_chunks": 0}
    
    # ë¬¸ì„œë³„ë¡œ ê·¸ë£¹í™”
    doc_info = {}
    for meta in metas:
        source = meta["source"]
        if source not in doc_info:
            doc_info[source] = {
                "filename": source,
                "path": meta.get("path", ""),
                "chunks": 0,
                "first_chunk_text": ""
            }
        doc_info[source]["chunks"] += 1
        
        # ì²« ë²ˆì§¸ ì²­í¬ì˜ ì¼ë¶€ í…ìŠ¤íŠ¸ë¥¼ ë¯¸ë¦¬ë³´ê¸°ë¡œ ì‚¬ìš©
        if doc_info[source]["chunks"] == 1 and meta["chunk_id"] == 0:
            chunk_index = meta["uid"]
            if chunk_index < len(texts):
                preview_text = texts[chunk_index][:200] + "..." if len(texts[chunk_index]) > 200 else texts[chunk_index]
                doc_info[source]["first_chunk_text"] = preview_text
    
    return {
        "documents": list(doc_info.values()),
        "total_documents": len(doc_info),
        "total_chunks": len(metas)
    }

@app.post("/ask")
def ask(req: AskReq):
    """ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤"""
    # ì„¸ì…˜ ê´€ë¦¬
    session_id = req.session_id or str(uuid.uuid4())
    if session_id not in sessions:
        sessions[session_id] = ConversationSession(session_id)
    
    session = sessions[session_id]
    
    # ì‚¬ìš©ì ì§ˆë¬¸ ì €ì¥
    session.add_message("user", req.question)
    
    # ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰
    hits = search_similar(req.question, k=TOP_K)
    
    if not client:
        answer = "âš ï¸ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    else:
        # LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        user_prompt = build_prompt(req.question, hits, session.get_context())
        
        # OpenAI API í˜¸ì¶œ
        completion = client.chat.completions.create(
            model="gpt-4o-mini",
            temperature=0.1,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user_prompt}
            ]
        )
        answer = completion.choices[0].message.content
    
    # ì‘ë‹µ ì €ì¥
    session.add_message("assistant", answer, hits)
    
    return {
        "answer": answer,
        "sources": hits,
        "session_id": session_id
    }

# === ì¦ë¶„ ì¸ë±ì‹± í•¨ìˆ˜ë“¤ (ì„œë²„ë¦¬ìŠ¤ì—ì„œëŠ” ë¹„í™œì„±í™”) ===
async def add_documents_to_index(file_paths: List[Path]):
    """ìƒˆ ë¬¸ì„œë“¤ì„ ê¸°ì¡´ ì¸ë±ìŠ¤ì— ì¶”ê°€ (ì„œë²„ë¦¬ìŠ¤ì—ì„œëŠ” ì§€ì›í•˜ì§€ ì•ŠìŒ)"""
    if IS_SERVERLESS:
        raise HTTPException(status_code=501, detail="ì„œë²„ë¦¬ìŠ¤ í™˜ê²½ì—ì„œëŠ” ë¬¸ì„œ ì¶”ê°€ê°€ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    
    # ê¸°ì¡´ ì½”ë“œëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë˜ ì„œë²„ë¦¬ìŠ¤ì—ì„œëŠ” ì‹¤í–‰í•˜ì§€ ì•ŠìŒ
    return 0

# ì„œë²„ë¦¬ìŠ¤ì—ì„œëŠ” íŒŒì¼ ì—…ë¡œë“œ/ê´€ë¦¬ ê¸°ëŠ¥ ë¹„í™œì„±í™”
@app.get("/health")
def health_check():
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "status": "healthy",
        "environment": "serverless" if IS_SERVERLESS else "standard",
        "model_loaded": emb_model is not None,
        "index_loaded": index is not None,
        "documents_count": len(metas) if metas else 0
    }

if __name__ == "__main__":
    import uvicorn
    port = int(os.environ.get("PORT", 8003))
    uvicorn.run(app, host="0.0.0.0", port=port)
