# app.py
import os, json, asyncio
from pathlib import Path
import faiss
import numpy as np
from fastapi import FastAPI, HTTPException
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from pydantic import BaseModel
from sentence_transformers import SentenceTransformer
from dotenv import load_dotenv
from openai import OpenAI
import uuid
from datetime import datetime
from typing import List, Dict, Optional
from contextlib import asynccontextmanager
from fastapi import UploadFile, File, Header, Depends
import shutil
import tempfile
import subprocess
import os
from file_watcher import init_file_watcher, start_file_watcher, stop_file_watcher, get_watcher_status

# --- ì±—ë´‡ì˜ í•µì‹¬ ìì›(ëª¨ë¸, ì¸ë±ìŠ¤)ì„ ê´€ë¦¬í•˜ëŠ” lifespan í•¨ìˆ˜ ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    # ì„œë²„ê°€ ì‹œì‘ë  ë•Œ ì‹¤í–‰ë˜ëŠ” ë¶€ë¶„
    print("ğŸš€ ì„œë²„ ì‹œì‘! ì¸ë±ìŠ¤ì™€ ëª¨ë¸ì„ ë¹„ë™ê¸°ì ìœ¼ë¡œ ë¡œë“œí•©ë‹ˆë‹¤...")
    # ë¬´ê±°ìš´ ì‘ì—…ì„ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ì„œë²„ ì‹œì‘ì„ ë°©í•´í•˜ì§€ ì•ŠìŒ
    await load_resources()
    
    # íŒŒì¼ ì›Œì³ ì´ˆê¸°í™” ë° ì‹œì‘
    print("ğŸ‘ï¸ íŒŒì¼ ì›Œì³ ì´ˆê¸°í™” ì¤‘...")
    init_file_watcher(DATA_DIR, add_documents_to_index)
    start_file_watcher()
    print("âœ… íŒŒì¼ ì›Œì³ ì‹œì‘ë¨")
    
    yield
    # ì„œë²„ê°€ ì¢…ë£Œë  ë•Œ ì‹¤í–‰ë˜ëŠ” ë¶€ë¶„ (ì •ë¦¬ ì½”ë“œ)
    print("ğŸ›‘ íŒŒì¼ ì›Œì³ ì¤‘ì§€ ì¤‘...")
    stop_file_watcher()
    print("ğŸ‘‹ ì„œë²„ ì¢…ë£Œ.")

async def load_resources():
    """AI ëª¨ë¸ ë° ì¸ë±ìŠ¤ íŒŒì¼ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜"""
    global index, metas, texts, emb_model
    try:
        index = faiss.read_index(str(INDEX_DIR / "faiss.index"))
        with open(INDEX_DIR / "meta.json", "r", encoding="utf-8") as f:
            store = json.load(f)
        metas = store["metas"]
        texts = store["texts"]
        emb_model = SentenceTransformer(EMB_MODEL_NAME)
        print("âœ… ì¸ë±ìŠ¤ì™€ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
    except Exception as e:
        print(f"âŒ ì¸ë±ìŠ¤/ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# API í‚¤ í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ
OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')

if not OPENAI_API_KEY or OPENAI_API_KEY == "YOUR_API_KEY_HERE":
    print("âš ï¸ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    client = None
else:
    client = OpenAI(api_key=OPENAI_API_KEY)

INDEX_DIR = Path("index")
DATA_DIR = Path("data")
EMB_MODEL_NAME = "jhgan/ko-sroberta-multitask"
TOP_K = 10
SIMILARITY_THRESHOLD = 0.1

# í‚¤ì›Œë“œ í™•ì¥ ë§µ
KEYWORD_EXPANSION = {
    "ë‹¤ìë…€ê°€ì •": ["ë‹¤ìë…€", "ì…‹ì§¸ì•„ì´", "3ìë…€", "3ëª… ì´ìƒ", "ë§ì€ ìë…€"], 
    "ë‹¤ìë…€": ["ë‹¤ìë…€ê°€ì •", "ì…‹ì§¸ì•„ì´", "3ìë…€", "3ëª… ì´ìƒ"],
    "í˜œíƒ": ["ì§€ì›", "ë³´ì¡°", "ê¸‰ì—¬", "ìˆ˜ë‹¹", "í• ì¸", "ê°ë©´", "ìš°ëŒ€"], 
    "ì§€ì›": ["í˜œíƒ", "ë³´ì¡°", "ê¸‰ì—¬", "ìˆ˜ë‹¹", "ì§€ì›ê¸ˆ"],
    "ì„ì‹ ": ["ì„ì‚°ë¶€", "ì˜ˆë¹„ë§˜", "ì‚°ëª¨", "ì„ì‹ ë¶€"], 
    "ì¶œì‚°": ["ë¶„ë§Œ", "í•´ì‚°", "ì‹ ìƒì•„", "ì‚°í›„ì¡°ë¦¬"],
    "ìœ¡ì•„": ["ì–‘ìœ¡", "ìë…€ëŒë´„", "ë³´ìœ¡", "ìœ¡ì•„íœ´ì§"], 
    "ë³´ìœ¡": ["ì–´ë¦°ì´ì§‘", "ìœ ì¹˜ì›", "ë†€ì´ë°©", "ìœ¡ì•„", "ì–‘ìœ¡"],
    "í•œë¶€ëª¨": ["í•œë¶€ëª¨ê°€ì •", "ë¯¸í˜¼ëª¨", "í¸ë¶€ëª¨", "ì¡°ì†ê°€ì •"]
}

# === ì¸ë±ìŠ¤ ë° ëª¨ë¸ ë³€ìˆ˜ ì´ˆê¸°í™” ===
index = None
metas = []
texts = []
emb_model = None

# FastAPI ì•± ìƒì„± ì‹œ lifespan ì—°ê²°
app = FastAPI(
    title="ì§€ëŠ¥í˜• ë³µì§€ ìƒë‹´ ì±—ë´‡",
    description="ë¬¸ì„œ ê¸°ë°˜ì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” RAG ì±—ë´‡ì…ë‹ˆë‹¤.",
    lifespan=lifespan
)

app.mount("/static", StaticFiles(directory="frontend"), name="static")
app.mount("/admin", StaticFiles(directory="admin"), name="admin")

# ê´€ë¦¬ì ì„¤ì •
ADMIN_PASSWORD = "admin123"  # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” í™˜ê²½ë³€ìˆ˜ë¡œ ê´€ë¦¬

def verify_admin_password(x_admin_password: Optional[str] = Header(None)):
    """ê´€ë¦¬ì ê¶Œí•œ í™•ì¸"""
    if x_admin_password != ADMIN_PASSWORD:
        raise HTTPException(status_code=403, detail="ê´€ë¦¬ì ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤")
    return True

# ì„¸ì…˜ ê´€ë¦¬
class ConversationMessage(BaseModel):
    role: str
    content: str
    timestamp: datetime
    sources: Optional[List[Dict]] = None

class ConversationSession:
    def __init__(self, session_id: str):
        self.session_id = session_id
        self.messages: List[ConversationMessage] = []
        self.created_at = datetime.now()
    
    def add_message(self, role: str, content: str, sources: Optional[List[Dict]] = None):
        self.messages.append(ConversationMessage(
            role=role, 
            content=content, 
            timestamp=datetime.now(), 
            sources=sources
        ))
    
    def get_context(self, max_messages: int = 4) -> str:
        """ìµœê·¼ ëŒ€í™” ë‚´ìš©ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ ë°˜í™˜"""
        recent_messages = self.messages[-max_messages:]
        context_parts = []
        for msg in recent_messages:
            if msg.role == "user":
                context_parts.append(f"ì‚¬ìš©ì: {msg.content}")
            else:
                context_parts.append(f"ìƒë‹´ì‚¬: {msg.content}")
        return "\n".join(context_parts)

sessions: Dict[str, ConversationSession] = {}

# API ëª¨ë¸
class AskReq(BaseModel):
    question: str
    session_id: Optional[str] = None

class NewSessionResponse(BaseModel):
    session_id: str
    message: str

def expand_query(query: str) -> str:
    """ì§ˆë¬¸ì„ í™•ì¥í•˜ì—¬ ë” ë‚˜ì€ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì–»ê¸°"""
    expanded_terms = []
    query_lower = query.lower()
    
    for keyword, synonyms in KEYWORD_EXPANSION.items():
        if keyword in query_lower:
            expanded_terms.extend(synonyms)
        for synonym in synonyms:
            if synonym in query_lower and keyword not in expanded_terms:
                expanded_terms.append(keyword)
                expanded_terms.extend([s for s in synonyms if s != synonym])
    
    if expanded_terms:
        unique_terms = list(set(expanded_terms))
        return f"{query} {' '.join(unique_terms[:5])}"
    return query

def search_similar(query: str, k=TOP_K):
    """ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤"""
    if not index or not emb_model:
        raise HTTPException(status_code=503, detail="ëª¨ë¸/ì¸ë±ìŠ¤ê°€ ì•„ì§ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
    
    # ì§ˆë¬¸ í™•ì¥
    expanded_query = expand_query(query)
    
    # ì„ë² ë”© ë° ê²€ìƒ‰
    q_emb = emb_model.encode([expanded_query], normalize_embeddings=True).astype("float32")
    D, I = index.search(q_emb, k)
    
    results = []
    for i, score in zip(I[0], D[0]):
        if score >= SIMILARITY_THRESHOLD:  # ìœ ì‚¬ë„ ì„ê³„ê°’ ì´ìƒë§Œ í¬í•¨
            results.append({
                "text": texts[i],
                "source": metas[i]["source"],
                "chunk_id": metas[i]["chunk_id"],
                "score": float(score)
            })
    
    return results

SYSTEM_PROMPT = (
    "ë‹¹ì‹ ì€ ìƒì• ë³µì§€í”Œë«í¼ì˜ ì „ë¬¸ ë³µì§€ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ì œê³µëœ 'ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸'ë¥¼ ì§€ëŠ¥ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ë‹µë³€í•˜ì‹­ì‹œì˜¤.\n\n"
    "## ğŸ” ì •ì±… ë¶„ë¥˜ ë° ìš°ì„ ìˆœìœ„ ë¶„ì„ ë°©ë²•:\n"
    "1. **ì „ìš© ì •ì±…**: íŠ¹ì • ëŒ€ìƒ(ë‹¤ìë…€ê°€ì •, í•œë¶€ëª¨ê°€ì • ë“±)ë§Œì„ ìœ„í•œ ì „ìš© ì§€ì› ì •ì±…\n"
    "2. **ìš°ëŒ€ ì •ì±…**: ì¼ë°˜ ì •ì±…ì—ì„œ íŠ¹ì • ëŒ€ìƒì—ê²Œ ìš°ëŒ€ í˜œíƒì„ ì œê³µí•˜ëŠ” ì •ì±…\n"
    "3. **ê´€ë ¨ ì •ì±…**: ê°„ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ ì •ì±…\n\n"
    "## ğŸ“‹ ë‹µë³€ êµ¬ì¡°í™” ì§€ì¹¨:\n"
    "**ë‹¤ìë…€ê°€ì • ì§€ì›ì •ì±…** ì§ˆë¬¸ ì‹œ:\n"
    "- ğŸ¯ **ë‹¤ìë…€ê°€ì • ì „ìš© ì •ì±…** (ìµœìš°ì„ )\n"
    "- ğŸ”– **ë‹¤ìë…€ê°€ì • ìš°ëŒ€ í˜œíƒ** (ì°¨ìˆœìœ„)\n\n"
    "**ì„ì‹ Â·ì¶œì‚° ì§€ì›ì •ì±…** ì§ˆë¬¸ ì‹œ:\n"
    "- ğŸ¯ **ì„ì‹ Â·ì¶œì‚° ì „ìš© ì •ì±…** (ìµœìš°ì„ )\n"
    "- ğŸ”– **ì„ì‹ Â·ì¶œì‚° ìš°ëŒ€ í˜œíƒ** (ì°¨ìˆœìœ„)\n\n"
    "**í•œë¶€ëª¨ê°€ì • ì§€ì›ì •ì±…** ì§ˆë¬¸ ì‹œ:\n"
    "- ğŸ¯ **í•œë¶€ëª¨ê°€ì • ì „ìš© ì •ì±…** (ìµœìš°ì„ )\n"
    "- ğŸ”– **í•œë¶€ëª¨ê°€ì • ìš°ëŒ€ í˜œíƒ** (ì°¨ìˆœìœ„)\n\n"
    "## âœ… ë‹µë³€ ê·œì¹™:\n"
    "- ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ ì•ˆì—ì„œë§Œ ë‹µë³€í•˜ê³ , ì •ì±…ì„ ë¶„ë¥˜ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ì œì‹œí•˜ì„¸ìš”.\n"
    "- ê° ì •ì±…ì˜ ëŒ€ìƒ, ë‚´ìš©, ì‹ ì²­ë°©ë²•ì„ ëª…í™•íˆ ìš”ì•½í•˜ê³ , ë¬¸ì¥ë§ˆë‹¤ [ì¶œì²˜: íŒŒì¼ëª…#ì²­í¬]ë¥¼ í‘œê¸°í•˜ì„¸ìš”.\n"
    "- ì „ìš© ì •ì±…ì„ ìš°ì„ ì ìœ¼ë¡œ ìƒì„¸íˆ ì„¤ëª…í•˜ê³ , ìš°ëŒ€ ì¡°ê±´ì€ ë³„ë„ë¡œ êµ¬ë¶„í•˜ì—¬ ì„¤ëª…í•˜ì„¸ìš”.\n"
    "- ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì€ 'ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.\n"
    "- ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ê°€ë…ì„± ìˆê²Œ ì‘ì„±í•˜ì„¸ìš”.\n"
    "- ë‹µë³€ ë§ˆì§€ë§‰ì— ì¶”ê°€ ì§ˆë¬¸ì„ ìœ ë„í•˜ëŠ” ì¹œê·¼í•œ ë©˜íŠ¸ë¥¼ í¬í•¨í•˜ì„¸ìš”."
)

def build_prompt(question: str, contexts: list[dict], conversation_history: str = ""):
    """ì§ˆë¬¸ê³¼ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ LLM í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±"""
    ctx_blocks = [f"[{r['source']}#{r['chunk_id']}]\n{r['text']}\n" for r in contexts]
    ctx_text = "\n---\n".join(ctx_blocks)
    
    instruction = (
        "ì§ˆë¬¸ì— ë§ëŠ” ì •í™•í•œ ì •ë³´ë¥¼ ì°¾ì•„ì„œ, ì •ì±… ì¢…ë¥˜(ì „ìš©/ìš°ëŒ€/ê´€ë ¨)ì— ë”°ë¼ ë¶„ë¥˜í•˜ê³  "
        "ìš°ì„ ìˆœìœ„ì— ë§ê²Œ ìƒì„¸íˆ ì„¤ëª…í•˜ì„¸ìš”."
    )
    
    user_parts = []
    if conversation_history.strip():
        user_parts.append(f"[ì´ì „ ëŒ€í™”]\n{conversation_history}\n")
    
    user_parts.extend([
        f"[í˜„ì¬ ì§ˆë¬¸]\n{question}\n",
        f"[ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸]\n{ctx_text}\n",
        f"[ì§€ì‹œì‚¬í•­]\n{instruction}\n"
    ])
    
    return "\n".join(user_parts)

@app.get("/")
async def read_root():
    return FileResponse("frontend/index.html")

@app.post("/new-session")
def create_new_session():
    """ìƒˆë¡œìš´ ëŒ€í™” ì„¸ì…˜ì„ ìƒì„±í•©ë‹ˆë‹¤"""
    session_id = str(uuid.uuid4())
    sessions[session_id] = ConversationSession(session_id)
    return NewSessionResponse(
        session_id=session_id,
        message="ìƒˆë¡œìš´ ìƒë‹´ ì„¸ì…˜ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. ê¶ê¸ˆí•œ ë³µì§€ ì •ì±…ì— ëŒ€í•´ ì§ˆë¬¸í•´ì£¼ì„¸ìš”!"
    )

@app.get("/sessions")
def list_sessions():
    """í˜„ì¬ í™œì„± ì„¸ì…˜ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤"""
    return {
        "sessions": [
            {
                "session_id": session.session_id,
                "created_at": session.created_at,
                "message_count": len(session.messages)
            }
            for session in sessions.values()
        ]
    }

@app.get("/session/{session_id}/history")
def get_session_history(session_id: str):
    """íŠ¹ì • ì„¸ì…˜ì˜ ëŒ€í™” ê¸°ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤"""
    if session_id not in sessions:
        raise HTTPException(status_code=404, detail="ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    session = sessions[session_id]
    return {
        "session_id": session_id,
        "messages": [
            {
                "role": msg.role,
                "content": msg.content,
                "timestamp": msg.timestamp,
                "sources": msg.sources
            }
            for msg in session.messages
        ]
    }

@app.get("/documents")
def get_indexed_documents():
    """ì¸ë±ìŠ¤ëœ ë¬¸ì„œ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤"""
    if not metas:
        return {"documents": [], "total_documents": 0, "total_chunks": 0}
    
    # ë¬¸ì„œë³„ë¡œ ê·¸ë£¹í™”
    doc_info = {}
    for meta in metas:
        source = meta["source"]
        if source not in doc_info:
            doc_info[source] = {
                "filename": source,
                "path": meta.get("path", ""),
                "chunks": 0,
                "first_chunk_text": ""
            }
        doc_info[source]["chunks"] += 1
        
        # ì²« ë²ˆì§¸ ì²­í¬ì˜ ì¼ë¶€ í…ìŠ¤íŠ¸ë¥¼ ë¯¸ë¦¬ë³´ê¸°ë¡œ ì‚¬ìš©
        if doc_info[source]["chunks"] == 1 and meta["chunk_id"] == 0:
            chunk_index = meta["uid"]
            if chunk_index < len(texts):
                preview_text = texts[chunk_index][:200] + "..." if len(texts[chunk_index]) > 200 else texts[chunk_index]
                doc_info[source]["first_chunk_text"] = preview_text
    
    return {
        "documents": list(doc_info.values()),
        "total_documents": len(doc_info),
        "total_chunks": len(metas)
    }

@app.post("/ask")
def ask(req: AskReq):
    """ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤"""
    # ì„¸ì…˜ ê´€ë¦¬
    session_id = req.session_id or str(uuid.uuid4())
    if session_id not in sessions:
        sessions[session_id] = ConversationSession(session_id)
    
    session = sessions[session_id]
    
    # ì‚¬ìš©ì ì§ˆë¬¸ ì €ì¥
    session.add_message("user", req.question)
    
    # ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰
    hits = search_similar(req.question, k=TOP_K)
    
    if not client:
        answer = "âš ï¸ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    else:
        # LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        user_prompt = build_prompt(req.question, hits, session.get_context())
        
        # OpenAI API í˜¸ì¶œ
        completion = client.chat.completions.create(
            model="gpt-4o-mini",
            temperature=0.1,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user_prompt}
            ]
        )
        answer = completion.choices[0].message.content
    
    # ì‘ë‹µ ì €ì¥
    session.add_message("assistant", answer, hits)
    
    return {
        "answer": answer,
        "sources": hits,
        "session_id": session_id
    }

# === ì¦ë¶„ ì¸ë±ì‹± í•¨ìˆ˜ë“¤ ===
async def add_documents_to_index(file_paths: List[Path]):
    """ìƒˆ ë¬¸ì„œë“¤ì„ ê¸°ì¡´ ì¸ë±ìŠ¤ì— ì¶”ê°€"""
    global index, metas, texts, emb_model
    
    if not emb_model:
        await load_resources()
    
    # ê¸°ì¡´ ë©”íƒ€ë°ì´í„°ì—ì„œ ìµœëŒ€ uid ì°¾ê¸°
    max_uid = max([meta["uid"] for meta in metas], default=-1)
    
    new_vectors = []
    new_metas = []
    new_texts = []
    
    from ingest import load_and_extract, chunk_text
    
    for file_path in file_paths:
        print(f"ğŸ“„ ì²˜ë¦¬ ì¤‘: {file_path.name}")
        
        # ì´ë¯¸ ì¸ë±ìŠ¤ëœ ë¬¸ì„œì¸ì§€ í™•ì¸
        if any(meta["source"] == file_path.name for meta in metas):
            print(f"âš ï¸ {file_path.name}ì€ ì´ë¯¸ ì¸ë±ìŠ¤ë¨. ê±´ë„ˆëœ€.")
            continue
        
        try:
            text = load_and_extract(file_path)
            if not text:
                print(f"âš ï¸ {file_path.name}: í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨")
                continue
                
            chunks = chunk_text(text)
            print(f"ğŸ“„ {file_path.name}: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
            
            for i, chunk in enumerate(chunks):
                max_uid += 1
                meta = {
                    "uid": max_uid,
                    "source": file_path.name,
                    "chunk_id": i,
                    "path": str(file_path.resolve())
                }
                new_metas.append(meta)
                new_texts.append(chunk)
        except Exception as e:
            print(f"âŒ {file_path.name} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            continue
    
    if new_texts:
        # ìƒˆ í…ìŠ¤íŠ¸ë“¤ ì„ë² ë”©
        print(f"ğŸ”„ {len(new_texts)}ê°œ ì²­í¬ ì„ë² ë”© ìƒì„± ì¤‘...")
        new_embeddings = emb_model.encode(new_texts, normalize_embeddings=True)
        new_embeddings = np.array(new_embeddings).astype("float32")
        
        # ê¸°ì¡´ ì¸ë±ìŠ¤ì— ì¶”ê°€
        index.add(new_embeddings)
        metas.extend(new_metas)
        texts.extend(new_texts)
        
        # ì¸ë±ìŠ¤ ì €ì¥
        save_index()
        print(f"âœ… {len(new_texts)}ê°œ ì²­í¬ê°€ ì¸ë±ìŠ¤ì— ì¶”ê°€ë¨")
        return len(new_texts)
    else:
        print("âš ï¸ ì¶”ê°€í•  ìƒˆë¡œìš´ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤")
        return 0

def save_index():
    """ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„° ì €ì¥"""
    INDEX_DIR.mkdir(exist_ok=True)
    faiss.write_index(index, str(INDEX_DIR / "faiss.index"))
    with open(INDEX_DIR / "meta.json", "w", encoding="utf-8") as f:
        json.dump({"metas": metas, "texts": texts}, f, ensure_ascii=False)

async def rebuild_full_index():
    """ì „ì²´ ì¸ë±ìŠ¤ ì¬êµ¬ì¶•"""
    print("ğŸ”„ ì „ì²´ ì¸ë±ìŠ¤ ì¬êµ¬ì¶• ì‹œì‘...")
    
    # ingest.py ì‹¤í–‰
    result = subprocess.run([
        "python", "ingest.py"
    ], capture_output=True, text=True, encoding="utf-8")
    
    if result.returncode == 0:
        # ì¸ë±ìŠ¤ ë‹¤ì‹œ ë¡œë“œ
        await load_resources()
        print("âœ… ì¸ë±ìŠ¤ ì¬êµ¬ì¶• ì™„ë£Œ")
        return True
    else:
        print(f"âŒ ì¸ë±ìŠ¤ ì¬êµ¬ì¶• ì‹¤íŒ¨: {result.stderr}")
        return False

def remove_document_from_index(filename: str):
    """íŠ¹ì • ë¬¸ì„œë¥¼ ì¸ë±ìŠ¤ì—ì„œ ì œê±°"""
    global index, metas, texts
    
    # í•´ë‹¹ ë¬¸ì„œì˜ ì¸ë±ìŠ¤ë“¤ ì°¾ê¸°
    indices_to_remove = []
    for i, meta in enumerate(metas):
        if meta["source"] == filename:
            indices_to_remove.append(i)
    
    if not indices_to_remove:
        return False
    
    # ì—­ìˆœìœ¼ë¡œ ì œê±° (ì¸ë±ìŠ¤ ë³€í™” ë°©ì§€)
    for i in reversed(indices_to_remove):
        del metas[i]
        del texts[i]
    
    # ì „ì²´ ì¸ë±ìŠ¤ ì¬êµ¬ì¶• í•„ìš” (FAISSëŠ” ê°œë³„ ë²¡í„° ì‚­ì œ ë¯¸ì§€ì›)
    return True

# === ê´€ë¦¬ì API ì—”ë“œí¬ì¸íŠ¸ë“¤ ===
@app.post("/admin/upload")
async def upload_documents(
    files: List[UploadFile] = File(...),
    _: bool = Depends(verify_admin_password)
):
    """ë¬¸ì„œ ì—…ë¡œë“œ ë° ì¸ë±ì‹±"""
    DATA_DIR.mkdir(exist_ok=True)
    uploaded_files = []
    
    try:
        for file in files:
            # íŒŒì¼ ì €ì¥
            file_path = DATA_DIR / file.filename
            with open(file_path, "wb") as buffer:
                shutil.copyfileobj(file.file, buffer)
            uploaded_files.append(file_path)
        
        # ì¦ë¶„ ì¸ë±ì‹±
        added_chunks = await add_documents_to_index(uploaded_files)
        
        return {
            "message": f"{len(uploaded_files)}ê°œ íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ. {added_chunks}ê°œ ì²­í¬ê°€ ì¸ë±ìŠ¤ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.",
            "files": [f.name for f in uploaded_files],
            "added_chunks": added_chunks
        }
    
    except Exception as e:
        # ì˜¤ë¥˜ ì‹œ ì—…ë¡œë“œëœ íŒŒì¼ë“¤ ì •ë¦¬
        for file_path in uploaded_files:
            if file_path.exists():
                file_path.unlink()
        raise HTTPException(status_code=500, detail=f"ì—…ë¡œë“œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")

@app.delete("/admin/documents/{filename}")
async def delete_document(
    filename: str,
    _: bool = Depends(verify_admin_password)
):
    """ë¬¸ì„œ ì‚­ì œ"""
    try:
        # íŒŒì¼ ì‹œìŠ¤í…œì—ì„œ ì‚­ì œ
        file_path = DATA_DIR / filename
        if file_path.exists():
            file_path.unlink()
        
        # ì¸ë±ìŠ¤ì—ì„œ ì œê±°
        removed = remove_document_from_index(filename)
        if removed:
            # ì „ì²´ ì¬êµ¬ì¶• í•„ìš”
            await rebuild_full_index()
            return {"message": f"ë¬¸ì„œ '{filename}'ì´ ì‚­ì œë˜ê³  ì¸ë±ìŠ¤ê°€ ì¬êµ¬ì¶•ë˜ì—ˆìŠµë‹ˆë‹¤."}
        else:
            return {"message": f"ë¬¸ì„œ '{filename}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì‚­ì œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")

@app.post("/admin/rebuild-index")
async def rebuild_index_endpoint(_: bool = Depends(verify_admin_password)):
    """ì „ì²´ ì¸ë±ìŠ¤ ì¬êµ¬ì¶•"""
    try:
        success = await rebuild_full_index()
        if success:
            return {"message": "ì¸ë±ìŠ¤ ì¬êµ¬ì¶•ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."}
        else:
            raise HTTPException(status_code=500, detail="ì¸ë±ìŠ¤ ì¬êµ¬ì¶•ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì¬êµ¬ì¶• ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")

@app.post("/admin/clear-index")
async def clear_index_endpoint(_: bool = Depends(verify_admin_password)):
    """ì¸ë±ìŠ¤ ì´ˆê¸°í™”"""
    global index, metas, texts
    
    try:
        # ë°ì´í„° íŒŒì¼ë“¤ ì‚­ì œ
        if DATA_DIR.exists():
            for file in DATA_DIR.glob("*"):
                if file.is_file():
                    file.unlink()
        
        # ì¸ë±ìŠ¤ íŒŒì¼ë“¤ ì‚­ì œ
        if INDEX_DIR.exists():
            for file in INDEX_DIR.glob("*"):
                if file.is_file():
                    file.unlink()
        
        # ë©”ëª¨ë¦¬ ìƒì˜ ë°ì´í„° ì´ˆê¸°í™”
        index = None
        metas = []
        texts = []
        
        return {"message": "ëª¨ë“  ì¸ë±ìŠ¤ì™€ ë¬¸ì„œê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."}
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì´ˆê¸°í™” ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")

@app.get("/admin/watcher-status")
async def get_watcher_status_endpoint(_: bool = Depends(verify_admin_password)):
    """íŒŒì¼ ì›Œì³ ìƒíƒœ í™•ì¸"""
    try:
        status = get_watcher_status()
        return status
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ìƒíƒœ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")

@app.post("/admin/watcher/start")
async def start_watcher_endpoint(_: bool = Depends(verify_admin_password)):
    """íŒŒì¼ ì›Œì³ ì‹œì‘"""
    try:
        success = start_file_watcher()
        if success:
            return {"message": "íŒŒì¼ ì›Œì³ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤."}
        else:
            raise HTTPException(status_code=500, detail="íŒŒì¼ ì›Œì³ ì‹œì‘ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì›Œì³ ì‹œì‘ ì¤‘ ì˜¤ë¥˜: {str(e)}")

@app.post("/admin/watcher/stop")
async def stop_watcher_endpoint(_: bool = Depends(verify_admin_password)):
    """íŒŒì¼ ì›Œì³ ì¤‘ì§€"""
    try:
        success = stop_file_watcher()
        if success:
            return {"message": "íŒŒì¼ ì›Œì³ê°€ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤."}
        else:
            raise HTTPException(status_code=500, detail="íŒŒì¼ ì›Œì³ ì¤‘ì§€ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì›Œì³ ì¤‘ì§€ ì¤‘ ì˜¤ë¥˜: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    port = int(os.environ.get("PORT", 8003))
    uvicorn.run(app, host="0.0.0.0", port=port)